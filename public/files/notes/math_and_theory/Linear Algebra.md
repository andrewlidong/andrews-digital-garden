# **ğŸ“– Linear Algebra Overview**

Linear algebra is the study of **vectors, vector spaces, and linear transformations**. It is fundamental in **computer science, physics, engineering, machine learning, and applied mathematics**.

## **ğŸ“Œ 1. Vectors and Vector Spaces**

### **What is a Vector?**

A **vector** is an ordered list of numbers, usually written as:

v=[v1v2v3]\mathbf{v} = \begin{bmatrix} v_1 \\ v_2 \\ v_3 \end{bmatrix}v=â€‹v1â€‹v2â€‹v3â€‹â€‹â€‹

Vectors represent **quantities with magnitude and direction** in **geometry** and **solutions to equations** in **algebra**.

### **Vector Spaces**

A **vector space** is a collection of vectors that satisfies:

1. **Closure under addition**: If u,v\mathbf{u}, \mathbf{v}u,v are in the space, then u+v\mathbf{u} + \mathbf{v}u+v is also in the space.
2. **Closure under scalar multiplication**: If v\mathbf{v}v is in the space and ccc is a scalar, then cvc\mathbf{v}cv is in the space.
3. **Zero vector**: There exists a unique **zero vector** 0\mathbf{0}0 such that v+0=v\mathbf{v} + \mathbf{0} = \mathbf{v}v+0=v.
4. **Other properties**: Associativity, commutativity, distributivity, etc.

#### **Examples of Vector Spaces**

- Rn\mathbb{R}^nRn, the space of all nnn-dimensional vectors.
- **Polynomial space**: All polynomials of degree â‰¤n\leq nâ‰¤n.
- **Matrix space**: All mÃ—nm \times nmÃ—n matrices.

## **ğŸ“Œ 2. Linear Transformations and Matrices**

### **What is a Linear Transformation?**

A **linear transformation** is a function between vector spaces that preserves addition and scalar multiplication:

T(au+bv)=aT(u)+bT(v)T(a\mathbf{u} + b\mathbf{v}) = aT(\mathbf{u}) + bT(\mathbf{v})T(au+bv)=aT(u)+bT(v)

Every linear transformation can be written as a **matrix multiplication**.

### **Matrices as Linear Maps**

A **matrix** represents a linear transformation. If:

A=[1234],v=[xy]A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}, \quad \mathbf{v} = \begin{bmatrix} x \\ y \end{bmatrix}A=[13â€‹24â€‹],v=[xyâ€‹]

then multiplying AAA and v\mathbf{v}v gives a new vector:

Av=[1234][xy]=[x+2y3x+4y]A\mathbf{v} = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} x + 2y \\ 3x + 4y \end{bmatrix}Av=[13â€‹24â€‹][xyâ€‹]=[x+2y3x+4yâ€‹]

This shows how AAA **transforms** v\mathbf{v}v.

## **ğŸ“Œ 3. Matrix Operations**

### **Matrix Addition and Multiplication**

- **Addition**: Matrices of the same size are added **element-wise**.
- **Multiplication**: If AAA is mÃ—nm \times nmÃ—n and BBB is nÃ—pn \times pnÃ—p, the product ABABAB is **defined** and results in an mÃ—pm \times pmÃ—p matrix.

### **Identity and Inverse Matrices**

- **Identity Matrix III**: The matrix equivalent of **1** in multiplication:
    
    I=[1001]I = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}I=[10â€‹01â€‹]
    
    For any matrix AAA, AI=AAI = AAI=A.
    
- **Inverse Matrix Aâˆ’1A^{-1}Aâˆ’1**: If AAâˆ’1=IAA^{-1} = IAAâˆ’1=I, then Aâˆ’1A^{-1}Aâˆ’1 **"undoes"** the transformation of AAA.
    

---

## **ğŸ“Œ 4. Determinants and Eigenvalues**

### **Determinant**

The **determinant** of a square matrix measures **scaling and invertibility**:

detâ¡(A)=adâˆ’bc\det(A) = ad - bcdet(A)=adâˆ’bc

For a matrix AAA:

- If detâ¡(A)=0\det(A) = 0det(A)=0, the matrix is **singular** (not invertible).
- If detâ¡(A)â‰ 0\det(A) \neq 0det(A)î€ =0, the matrix is **invertible**.

### **Eigenvalues and Eigenvectors**

For a square matrix AAA, an **eigenvector** v\mathbf{v}v satisfies:

Av=Î»vA\mathbf{v} = \lambda \mathbf{v}Av=Î»v

where Î»\lambdaÎ» is an **eigenvalue**.

- Eigenvalues tell us **how much vectors are stretched or shrunk**.
- Eigenvectors remain in the **same direction** after transformation.

#### **Example: Eigenvalues of a 2Ã—2 Matrix**

For A=[3102]A = \begin{bmatrix} 3 & 1 \\ 0 & 2 \end{bmatrix}A=[30â€‹12â€‹], solve:

detâ¡(Aâˆ’Î»I)=0\det(A - \lambda I) = 0det(Aâˆ’Î»I)=0

to find Î»\lambdaÎ», the eigenvalues.

### **Why This Matters**

- Used in **PCA (Principal Component Analysis)** for data compression.
- Essential in **differential equations, physics, and computer graphics**.
## **ğŸ“Œ 5. Basis and Dimension**

### **What is a Basis?**

A **basis** of a vector space is a **set of linearly independent vectors** that **span the entire space**.

- The number of basis vectors is the **dimension** of the space.
    
- Example: The standard basis of R3\mathbb{R}^3R3 is:
    
    e1=[100],e2=[010],e3=[001]\mathbf{e}_1 = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \quad \mathbf{e}_2 = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, \quad \mathbf{e}_3 = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}e1â€‹=â€‹100â€‹â€‹,e2â€‹=â€‹010â€‹â€‹,e3â€‹=â€‹001â€‹â€‹

### **Linear Independence**

A set of vectors {v1,v2,â€¦,vn}\{ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n \}{v1â€‹,v2â€‹,â€¦,vnâ€‹} is **linearly independent** if:

c1v1+c2v2+â‹¯+cnvn=0c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + \dots + c_n\mathbf{v}_n = 0c1â€‹v1â€‹+c2â€‹v2â€‹+â‹¯+cnâ€‹vnâ€‹=0

only when **all** ci=0c_i = 0ciâ€‹=0.

### **Why This Matters**

- Basis vectors let us **represent and analyze** vectors efficiently.
- Changing bases simplifies computations (e.g., diagonalization).

---

## **ğŸ“Œ 6. Orthogonality and Inner Product Spaces**

### **Dot Product (Inner Product)**

The **dot product** of two vectors u,v\mathbf{u}, \mathbf{v}u,v is:

uâ‹…v=u1v1+u2v2+â‹¯+unvn\mathbf{u} \cdot \mathbf{v} = u_1 v_1 + u_2 v_2 + \dots + u_n v_nuâ‹…v=u1â€‹v1â€‹+u2â€‹v2â€‹+â‹¯+unâ€‹vnâ€‹

- If uâ‹…v=0\mathbf{u} \cdot \mathbf{v} = 0uâ‹…v=0, then u\mathbf{u}u and v\mathbf{v}v are **orthogonal** (perpendicular).

### **Orthogonal Projections**

Given a vector v\mathbf{v}v, its **projection** onto u\mathbf{u}u is:

projuv=uâ‹…vuâ‹…uu\text{proj}_{\mathbf{u}} \mathbf{v} = \frac{\mathbf{u} \cdot \mathbf{v}}{\mathbf{u} \cdot \mathbf{u}} \mathbf{u}projuâ€‹v=uâ‹…uuâ‹…vâ€‹u

This is useful in **least squares regression** and **computer graphics**.

### **Why This Matters**

- **Gram-Schmidt Process**: Converts any basis into an **orthonormal basis**.
- **QR Decomposition**: Factorizes matrices into simpler components.