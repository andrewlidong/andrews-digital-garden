Distributed **training**

Training big ML models is the elephant in the room. So everyone starts by talking about compute, but it ends up being mostly about networking. Hello, data movement!

[Allreduce (2017)](https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/)•[How to Train Really Large Models on Many GPUs? (2021)](https://lilianweng.github.io/posts/2021-09-25-train-large/)•[Everything about Distributed Training and Efficient Finetuning (2024)](https://sumanthrh.com/post/distributed-and-efficient-finetuning/)•[Tensor Parallelism with jax.pjit (2022)](https://irhum.github.io/blog/pjit/)